---
title: "Intro to Multilevel Models"
author: "Matthew Drury"
output: html_document
---

This document presents an introduction to multilevel models, with a focus on
the subclass of random intercept models. We motivate the discussion by
highlighting some issues that present in classical linear regression, and focus
on how multilevel models offer solutions.  In the last section, we discuss how
multilevel models can be used to model situation where the levels of a
categorical variable can be grouped into natural classes, where we expect the
relationship with the response to factor in variation between groups, and
within the groups themselves. 

Linear Models and Noisy Coefficients
------------------------------------

Classical linear models (and, by extension, glms), while exceedingly useful in
many situations, deal very badly with high dimensional categorical variables.
To demonstrate this phenomena, consider the following procedure, which
regresses a normally distributed response $y$ against a totally independent
categorical predictor $X$

```{r}
estiamte_lm_random_classes <- function(N, N_classes) {
  X <- factor(sample(1:N_classes, N, replace=TRUE))
  y <- rnorm(N)
  M <- lm(y ~ X)
  coef(M)
}
```

If we simulate this procedure many times, we can see how the linear model
assigns parameter estimates to each of the categorical levels in $X$

```{r}
# Replicate fitting an estimator N times
replicate_models <- function(N, N_classes, estimator, N_replicates) {
  model_coefs <- replicate(N_replicates, estimator(N, N_classes))
  model_coefs_melted <- data.frame(
    model_id = rep(1:N_replicates, each=N_classes),
    coef = as.vector(model_coefs)
  )
  model_coefs_melted
}
# 10000 replications of the independent y, random class model
model_coefs_melted <- replicate_models(10000, 100, estiamte_lm_random_classes, 25)

library("ggplot2")
ggplot(data=model_coefs_melted, aes(x=coef)) +
  geom_histogram() +
  facet_wrap(~model_id) +
  labs(title="Parameter Estimates in Null Linear Model")
```

The distributions show significant variation in parameter estimates each time,
even though every true parameter is zero[^1].

[^1]: It would make an interesting exercise to work out precise distribution of each
parameter estimate in the model (as a function of the random $y$ and $X$).


Random Intercept Models
-----------------------

Random intercept models are a way to address the high variance that linear
models exhibit in the above situation.

Recall that we may express the data model underlying a linear regression as
follows

$$ Y \mid X \sim N(X \beta, \sigma) $$

That is, the conditional distribution of the response $Y$, when $X$ is held
constant, is normal with mean $X \beta$ (i.e. some linear function of $X$).
The standard deviation $\sigma$ of this marginal distribution is *not* a
function of $X$.

When fitting a linear model, the parameters $\beta$ and $\sigma$ are estimated
using maximum likelihood estimation.  As we have seen, when $X$ is a high
dimensional categorical vector, the linear model's estimates of $\beta$ are
plagued by a high variance even in the most simple situations.

A random intercept model alleviates this variance by adding an additional
assumption into the data model.  The parameter estimates themselves are assumed
to come from a normal distribution, whose parameter estimates the model also
estimates. This has a constraining effect on the estimated parameters, which
often results in more predictive models, and more discerning inference.

To be precise, let's set down some notation.  Let $\alpha_{[i]}$ denote the
parameter to be estimated for class $i$, i.e one the parameter estimate
associated with one level of our many-leveled categorical variable.  Then the
defining relations of a random intercept model are

$$ Y \mid X \sim N(\alpha_{[i]}, \sigma) $$
$$ \alpha \sim N(\mu_{\alpha}, \sigma_{\alpha}) $$

The first relation is just a restatement of the linear regression data model.
The second models the parameters in the linear model as *themselves*  random
variables, sampled from a normal distribution whose mean and standard deviation
are also to be estimated.

In the context of a multilevel model

  - $\sigma$ is called the *residual standard deviation*.
  - The $\alpha_{[i]}$ are called the *random intercepts*.
  - The $\mu_{\alpha}$ is called the *fixed intercept*, we will describe this
    in more detail in the next section.
  - The $\sigma_{\alpha}$ is called the *class standard deviation*.


Fitting Random Intercept Models in R
------------------------------------

In this section we continue to explore the example above.  To set up

```{r}
set.seed(154)
N <- 10000
N_classes <- 100
df <- data.frame(
  X <- factor(sample(1:N_classes, N, replace=TRUE)),
  y <- rnorm(N)
)

```

The classical linear model above is expressed in R using the built in `lm`
function

```{r}
M_lm <- lm(y ~ X, data = df)
```

Fitting random intercept models in R is easy with `lmer` function in the `lme4`
library (`lmer` is short for "linear mixed effects regression")

```{r message=FALSE, warning=FALSE}
library("lme4")
M_lmer <- lmer(y ~ (1 | X), data = df)
```

The mean of the random intercepts $\alpha$ was estimated as a global intercept
term.  If it's confusing why the random intercept mean $\mu_{\alpha}$ would
appear as a global intercept, just observe that the model definition could just
as well have been expressed as

$$ Y \mid X \sim N(\mu_{\alpha} + \alpha_{[i]}, \sigma I) $$
$$ \alpha \sim N(0, \sigma_{\alpha}) $$

which illustrates the general point that even the most simple multilevel models
may be expressed in many equivalent ways.  This global intercept term is our
first example of a *fixed effect*[^2].  It can be accessed with 

```{r}
fixef(M_lmer)
```

The collection of estimated random intercepts $\hat \alpha$ can be retrieved
using the `ranef` assessor function, which returns a list of data frames.

```{r}
head(ranef(M_lmer)$X)
```

The estimated standard deviation of these intercepts $\hat \sigma_{\alpha}$ can
be accessed (that is a bit too complicated in this authors opinion) as

```{r}
attributes(VarCorr(M_lmer)$X)$stddev
```

Finally, the residual standard deviation $\hat \sigma$ is

```{r}
sigma(M_lmer)
```

From these values, it looks like `lmer` got the model quite right.  The true
model is

$$ Y \mid X \sim N(0, 1) $$

so the correct parameters would be $\hat \alpha_i = 0$ for all $i$, $\hat
\sigma_{\alpha} = 0$, and $\hat \sigma = 1$.  We were very close [^3]!

To make sure we didn't just get lucky, we can repeat this entire procedure
many times, and then plot the resulting histograms of estimated random effects

```{r message=FALSE, warning=FALSE}
set.seed(154)
estiamte_lmer_random_classes <- function(N, N_classes) {
  df <- data.frame(
    x = factor(sample(1:N_classes, N, replace=TRUE)),
    y = rnorm(N)
  )
  M <- lmer(y ~ (1 | x), data = df)
  ranef(M)$x[, 1]
}

model_coefs_melted <- replicate_models(10000, 100,
                                       estiamte_lmer_random_classes, 25)

ggplot(data=model_coefs_melted, aes(x=coef)) +
  geom_histogram() +
  facet_wrap(~model_id) +
  labs(title="Parameter Estimates in Null Random Intercept Model")
```

The random intercept model does much better than the linear model.  It often
finds the exact truth ($\hat \sigma_{\alpha} = 0$), and otherwise assigns a
very small standard deviation to the distribution of random intercepts.

To emphasize the point, Here's a side by size comparison of the intercepts in
one of the models returned from `lme` vs the same returned from `lmer`

```{r message=FALSE, warning=FALSE}
set.seed(154)
df <- data.frame(
  x = factor(sample(1:N_classes, N, replace=TRUE)),
  y = rnorm(N)
)

M_lm <- lm(y ~ x, data = df)
M_lmer <- lmer(y ~ (1 | x), data = df)

plot_data <- data.frame(coef = c(coef(M_lm), ranef(M_lmer)$x[, 1]),
                        type = rep(c("lm", "lmer"), each = N_classes))

ggplot(aes(x = coef, fill = type), data = plot_data) + 
  geom_histogram() + 
  facet_wrap(~ type) +
  labs(title="Linear and Random Intercept Null Model Coefficients (Same Scale)")
```

The random intercept controls the untamed variance of the linear model.

[^2]: Fixed effects have have no a priori assumed distribution, as opposed to *random
effects*, for which we do make such an assumption.

[^3]: Of course, we cant say that we were "close" in an absolute sense, but the
ratio $\frac{\sigma_{\alpha}}{\sigma}$ of the group standard deviation to the
residual standard deviation *is* meaningful, and the model did well in this
respect.


Fixed Effect Terms
------------------

Above we saw that the intercept in a random intercept model is estimated as a
*fixed effect*.  We can add other fixed effects to a multilevel model.

For example, we can estimate a model with an intercept and linear term as fixed
effects, and a random intercept (as a *random effect*.  

In detail, we can write out this model as

$$ Y \mid X \sim N(\beta_0 + \beta_1 x + \alpha_{[i]}, \sigma) $$
$$ \alpha \sim N(0, \sigma_{\alpha}) $$

Note that this is completely equivalent to

$$ Y \mid X \sim N( \alpha_{[i]}, \sigma) $$
$$ \alpha \sim N(\beta_0 + \beta_1 x, \sigma_{\alpha}) $$

which again illustrates a multilevel model can be expressed in multiple ways.

This model is also easy to estimate with `lmer`

```{r}
set.seed(154)
df <- data.frame(
  t = runif(N),
  X = factor(sample(1:N_classes, N, replace=TRUE))
)
df$y <- df$t + rnorm(N)
M_lmer <- lmer(y ~ t + (1 | X), data = df)
```

Now there is both a fixed effect intercept and a fixed effect slope

```{r}
fixef(M_lmer)
```

We can check out the random effects just like in the simpler model

```{r}
attributes(VarCorr(M_lmer)$X)$stddev
```

Again, it looks like this model captures the essence of the situation.  We can
simulate many times to be sure

```{r echo=FALSE}
estimate_lmer_with_fixed <- function(N, N_classes) {
  df <- data.frame(
    t = runif(N),
    x = factor(sample(1:N_classes, N, replace=TRUE))
  )
  df$y <- df$t + rnorm(N)
  M <- lmer(y ~ t + (1 | x), data = df)
  M
}

set.seed(154)
N_replicates <- 9
N_classes <- 100
lmers_fixed_effects <- replicate(
  N_replicates,
  fixef(estimate_lmer_with_fixed(10000, N_classes))
)

# Evaluate the fixed effects part of the model on a grid of points on [0, 1]
N_grid_points <- 4
grid_matrix <- cbind(rep(1, N_grid_points), seq(0, 1, length = N_grid_points))
fixef_preds <- grid_matrix %*% lmers_fixed_effects
fixef_preds_melted <- data.frame(
  model_id = factor(rep(1:N_replicates, each = N_grid_points)),
  x = rep(seq(0, 1, length = N_grid_points), N_replicates),
  fix_effects_preds = as.vector(fixef_preds)
)
p_fixef <- ggplot(data = fixef_preds_melted, aes(x = x, y = fix_effects_preds)) +
                  geom_line(aes(group = model_id), alpha = .25) +
                  geom_abline(intercept = 0, slope = 1, color="red") +
                  labs(title="Fix Effects Estimates")


# Plot the random effect distribution from the same model
set.seed(154)
lmers_rand_effects <- replicate(
  N_replicates,
  ranef(estimate_lmer_with_fixed(10000, N_classes))$x[,1]
)

rand_effects_melted <- data.frame(
  model_id = rep(1:N_replicates, each = N_classes),
  coef = as.vector(lmers_rand_effects)
)

p_ranef <- ggplot(data=rand_effects_melted, aes(x=coef)) +
           geom_histogram() +
           facet_wrap(~model_id) +
           labs(title="Random Effects Estimates")
```

```{r message=FALSE, warning=FALSE}
# I've suppressed the busywork involved in making these plots.
library("gridExtra")
grid.arrange(p_fixef, p_ranef, ncol=2)
```

It should be clear at this point that we could generalize to *random slopes* as
well.  Experimentation here is left to the reader, but we will mention that a
random slope model can be fit in `lmer` with

```{r eval=FALSE}
lmer(y ~ (1 + t | X), data = ...)
```


Nested Classes
--------------

To understand why "Multilevel Models" is an appropriate name[^4] for this type of
model, lets construct some data with two types of predictors:

  - `class` is a categorical predictor with `N_class` levels.
  - `subclass` is a another categorical predictor that bears a parent-child
    relationship to `class`.  That is, each level of `class` has `N_subclass`
    associated levels of `subclass`.
  
Many times, in situations like this, we would expect the influence of
`subclass` on some response variable to be best expressed an adjustment to the
influence of `class`.  Said another way, the levels of `class` may act as
cluster labels for the levels of `subclass` within the context of a regression.

We can express this idea as a multilevel model by hypothesizing the following

$$ Y \mid X \sim N(\mu_{\text{class}} + \alpha_{[i]} + \beta_{[ij]}, \sigma) $$
$$ \alpha \sim N(0, \sigma_{\alpha}) $$
$$ \beta_{[i*]} \sim N(0, \sigma_{\alpha_{[i]}}) $$

The notation is starting to get difficult[^5].  Here $\alpha$ represents the
`class` random intercepts, and $\beta$ represents the `subclass` random
intercepts.  Informally, we are saying that the `class` intercepts should
cluster around the fixed intercept, and the `subclass` intercepts should
cluster around the corresponding parent `class` intercept.

Let's generate some data using this scheme.  First, `class` parameters[^6]

```{r}
N_class <- 9

class_parameters <- structure(
  rnorm(N_class, sd = 3),
  names = as.character(1:N_class)
)

head(class_parameters)
```

and then subclass parameters

```{r}
N_subclass <- 100

subclass_parameters <- structure(
  rnorm(N_class*N_subclass),
  # The transpose is to put things in an intuitive order
  names = as.vector(t(outer(1:N_class, 1:N_subclass, paste, sep=".")))
)

head(subclass_parameters)
```

We call the sum of a `class` and `subclass` parameter an `effective` parameter
(this is not a common term, but is useful for this discussion), it is this
value that forms the total effect size for the subclass.  We can visualize the
distribution of true effective parameters in our model, grouped by their
associated class

```{r meessage=FALSE, warning=FALSE}
# Get the class name from a subclass name. For example:
#   get_class_name("10.4") == "10"
get_class_name <- function(subclass_name) {
  strsplit(subclass_name, ".", fixed = TRUE)[[1]][1]
}

effective_parameters <- lapply(
  names(subclass_parameters),
  function(x) subclass_parameters[x] + class_parameters[get_class_name(x)]
)
names(effective_parameters) <- names(subclass_parameters)

parameters_df <- data.frame(
  effective_parameter = unlist(effective_parameters),
  class = rep(names(class_parameters), each = N_subclass),
  class_parameter = rep(class_parameters, each = N_subclass)
)

ggplot(data = parameters_df, aes(x = effective_parameter)) +
  geom_density(aes(fill = class), alpha = .25) +
  geom_rug(aes(x = class_parameter, colour = class)) +
  labs(title = "Distribution of True Effective Parameters")
```

It may be easier to visualize if we break out the individual subclass parameter
distributions into separate density plots

```{r meessage=FALSE, warning=FALSE}
ggplot(data = parameters_df, aes(x = effective_parameter)) +
  geom_density(aes(fill = class), alpha = .25) +
  geom_rug(aes(x = class_parameter, colour = class)) +
  facet_wrap(~ class) +
  labs(title = "Distribution of True Effective Parameters") 
```

Each group of `subclass` parameters takes a bell shaped distribution around the
`class` mean (which are marked on the rug plot below the x-axis).

The `lme4` library can fit the parameters in this model.  First, let's generate
some response data

```{r}
N_obs <- 10000

class <- as.character(sample(1:N_class, N_obs, replace = TRUE))
subclass <- as.character(sample(1:N_subclass, N_obs, replace = TRUE))
class_subclass <- paste(class, subclass, sep = ".")
y <- class_parameters[class] + subclass_parameters[class_subclass] + rnorm(N_obs, sd = 5)

class_subclass_df <- data.frame(
  class  = class,
  subclass = subclass,
  y = y
)

head(class_subclass_df)
```

The multilevel model we layed out above can be fit using the `:` operator in
`lme4` to express the parent-child relationship of `class` and `subclass`

```{r}
class_subclass_lmer <- lmer(y ~ (1 | class) + (1 | class:subclass),
                            data = class_subclass_df)
```

Breaking this down

  - The implied `1` fits a fixed effect overall intercept.
  - The `(1 | class)` term fits a random intercept for `class`.
  - The `(1 | class:subclass)` fits a random intercept for `subclass`, but with
    each group of `subclass` estimates centered around the estimated intercept
    for the parent `class`.

Let's look into how well this model captures reality.  First, we plot the
estimated and actual class means, along with the standard error of each
estimated class mean

```{r message=FALSE, warning=FALSE}
library("arm") # Contains extra functions for manipulating lmer objects.
class_means_df <- data.frame(
  class_id = names(class_parameters),
  true_class_means = class_parameters,
  estimated_class_means = fixef(class_subclass_lmer) + 
                          ranef(class_subclass_lmer)$class[, 1],
  # The intercept and class means are assumed to vary independently here.
  upper_se = fixef(class_subclass_lmer) + 
             ranef(class_subclass_lmer)$class[, 1] +
             sqrt(se.coef(class_subclass_lmer)$fixef^2 + 
                          se.coef(class_subclass_lmer)$class[, 1]^2),
  lower_se = fixef(class_subclass_lmer) +
             ranef(class_subclass_lmer)$class[, 1] -
             sqrt(se.coef(class_subclass_lmer)$fixef^2 + 
                          se.coef(class_subclass_lmer)$class[, 1]^2)
)

ggplot(data = class_means_df) +
  geom_point(aes(x = class_id, y = estimated_class_means, colour = "estimated")) +
  geom_pointrange(aes(x = class_id, y = estimated_class_means,
                      ymax = upper_se, ymin = lower_se, colour = "estimated"),
                  alpha = .5) +
  geom_point(aes(x = class_id, y = true_class_means, colour = "actual")) +
  scale_colour_manual(name = "type", values = c("black", "blue")) +
  labs(title = "Estimated vs. Actual Class Means")
```

We can also recreate the above density plots of the `subclass` parameters, this
time using the *estimated*, instead of true, values

```{r message=FALSE, warning=FALSE}
estimated_parameters_df <- data.frame(
  estimated_effective_parameter =
    rep(fixef(class_subclass_lmer), N_class*N_subclass) +
    rep(ranef(class_subclass_lmer)$class[, 1], each = N_subclass) +
    ranef(class_subclass_lmer)[["class:subclass"]][, 1],
  class = factor(rep(1:9, each = N_subclass))
)

ggplot(data = estimated_parameters_df, aes(x = estimated_effective_parameter)) +
  geom_density(aes(fill = class), alpha=.25) +
  labs(title = "Distribution of Estimated Effective Parameters")
```

Here that is side by side with the actual values

```{r message=FALSE, warning=FALSE}
actuals <- ggplot(data = parameters_df, aes(x = effective_parameter)) +
  geom_density(aes(fill = class), alpha=.25)

estimates <- ggplot(data = estimated_parameters_df,
                    aes(x = estimated_effective_parameter)) +
  geom_density(aes(fill = class), alpha = .25)

grid.arrange(actuals + scale_x_continuous(limits = c(-8, 8)),
             estimates + scale_x_continuous(limits = c(-8, 8)),
             nrow=2)
```

Here we see quite clearly the qualitative nature of a nested multilevel model:
the `suclass` intercepts have been shrunk towards their corresponding class
means.  It is in this way that multilevel regression solves the variance
problem of linear regression.

This is all quite similar to *ridge regression*, another popular extension to
the classical linear regression framework.  In ridge regression we estimate
parameters by maximizing the penalized least squares criterion

$$ \sum_i \left( y_i - \sum_j x_{ij} \beta_j \right)^2 + \lambda \sum_j \beta_j^2 $$

There are two significant conceptual differences

  - In ridge regression, all the parameters in the model are shrunk towards zero,
  in multilevel regression, they are shrunk towards the class means.
  - In ridge regression, the intensity of shrinkage is controlled by the free
  parameter $\lambda$, which must be tuned by some meta-procedure (cross 
  validation is popular).  In a multilevel model, the appropriate level of
  shrinkage is estimated as part of the fitting algorithm.


To drive home the point, its worth seeing what would happen here if we fit a
classical linear model with no regularization or shrinkage

```{r message=FALSE, warning=FALSE}
subclass_lm <- lm(y ~ -1 + factor(subclass) : factor(class), data = class_subclass_df)
estimated_parameters_lm <- coef(subclass_lm)

# Assign reasonable names to the coefficients in the correct order.
names(estimated_parameters_lm) <- as.vector(
  t(outer(1:N_class, 1:N_subclass, paste, sep="."))
)

subclass_lm_df <- data.frame(
  estimated_parameter = estimated_parameters_lm,
  class = rep(names(class_parameters), each = N_subclass)
)

estimates_lm <- ggplot(data = subclass_lm_df, aes(x = estimated_parameter)) + 
  geom_density(aes(fill = class), alpha = .25)
  
grid.arrange(actuals + scale_x_continuous(limits = c(-10, 10)),
             estimates_lm + scale_x_continuous(limits = c(-10, 10)),
             nrow=2)
```

As we would have guessed, here the in class variation of the `subclass` parameters
is too large (notice that we had to expand the x-axis to $(-10, 10)$ to capture
the full range).  This is a bad practice, as parameters supported by little
data will be those whose effects get overestimated, it's much more reasonable to
take account of our prior belief that effects cluster around the `class` mean, as
a multilevel model enforces.

[^4]: Sometimes the term "Hierarchical Models" is also used, or "Mixed-Effects
Models", the naming is inconsistent and confusing.

[^5]: It always does, eventually.  To make it worse, here's yet another way to
express the model:
$$ Y \mid X \sim N(\beta_{[ij]}, \sigma) $$
$$ \alpha \sim N(\mu_{\alpha}, \sigma_{\alpha}) $$
$$ \beta_{[i*]} \sim N(\alpha_{[i]}, \sigma_{\alpha_{[i]}}) $$

[^6]: We limit to $9$ class parameters to assist with visualizations.
