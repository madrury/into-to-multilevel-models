---
title: "Intro to Multilevel Models"
author: "Matthe Drury"
date: "February 28, 2016"
output: html_document
---

This document presents an introduction to multilevel models, with a focus on
random intercept models. We motivate the discussion by hghligting some issues
that present in classical linear regression, and focus on how multilevel models
offer solutions.

Linear Models and Noisy Coefficients
------------------------------------

Linear models, while exceedingly useful in many situations, deal very badly
with high dimensional catagorical varaibles.  To demonstrate this, consider the
following procedure, which regresses a normally distributed $Y$ against a
totally independent catagorical predictor $X$

```{r}
# Fit a linear model, regressing an independent response y against and a 
# random class varaible
estiamte_lm_random_classes <- function(N, N_classes) {
  X <- factor(sample(1:N_classes, N, replace=TRUE))
  y <- rnorm(N)
  M <- lm(y ~ X)
  coef(M)
}
```

If we simulate this procedure many times, we can see how the linear model
assign parameter estimates to this data in the long run.

```{r}
# Replicate fitting an estimator N times
replicate_models <- function(N, N_classes, estimator, N_replicates) {
  model_coefs <- replicate(N_replicates, estimator(N, N_classes))
  model_coefs_melted <- data.frame(
    model_id = rep(1:N_replicates, each=N_classes),
    coef = as.vector(model_coefs)
  )
  model_coefs_melted
}
# 10000 replications of the independent y, random class model
model_coefs_melted <- replicate_models(10000, 100, estiamte_lm_random_classes, 25)
```

The distributions show significant variation in parameter estiamtes each time,
although every true parameter is zero.

```{r message=FALSE, warning=FALSE}
library("ggplot2")
ggplot(data=model_coefs_melted, aes(x=coef)) +
  geom_histogram() +
  facet_wrap(~model_id) +
  labs(title="Parameter Estimates in Null Linear Model")
```

It would make an interesting excersize to work out precise distribution of each
parameter estimate in the model (as a function of the random $y$ and $X$), it
is quite non-zero.

Random Intercept Models
-----------------------

Random intercept models are a way to adress the high varaince that linear
models exhibit in teh above situation.

Recall that we may express the data model underlying a linear regression as
follows

$$ Y \mid X \sim N(X \beta, \sigma I) $$

That is, the conditional distribution of the reponse $Y$, when $X$ is held
constant, is (multivariate) normal with mean $X \beta$.  The individual samples
from $Y$ are independent, and the std-deviation of any one sample (again, given
an $X$ held constant) is $\sigma$.  When fitting a linear model, the parameters
$\beta$ and $\sigma$ are esimtated using maximum liklihood estimation.  As we
have seen, when $X$ is a high dimensional catagorical vector, the linear
model's estiamtes of $\beta$ are plauged by a high variance even in the most
simple situations.

A random intercept model alleviates this varaince by adding an additional
assumption into the data model.  The parameter estimates themselves (refered to
in this context as *random intercepts*) are assumed to come from a normal
distribution, whose parameter estiamtes the model also estiamtes. This has a
constraining effect on the estimated parameters, which often results in more
predictive models.

To be precise, let's set down some notation.  Let $\alpha_{[i]}$ denotre the
parameter to be estiamted for class $i$, i.e one the parameter estiamte
associated with one level of our many-leveled catagorical variable.  Then the
defining relations of a random intercept model are

$$ Y \mid X \sim N(\alpha_{[i]}, \sigma I) $$
$$ \alpha \sim N( \mu_{\alpha}, \sigma_{\alpha}) $$

The first relation is just a restatement of the linear regression data model.
The second models the parameters in the linear model as *themselves*  random
varaibles, sampled from a normal distribution whose mean and standard deviation
are also to be estiamted.

Fitting Random Intercept Models in R
------------------------------------

Fitting random intercept models in R is easy with the `lme4` library.

The linear model above is expressed in R as

```{r}
set.seed(154)
N <- 10000
N_classes <- 100
df <- data.frame(
  X <- factor(sample(1:N_classes, N, replace=TRUE)),
  y <- rnorm(N)
)
M_lm <- lm(y ~ X, data = df)
```

The random intercept model is fit using the `lmer` function (for *linear mixed
effects regression*)

```{r message=FALSE, warning=FALSE}
library("lme4")
M_lmer <- lmer(y ~ (1 | X), data = df)
```

We can pull the components described above out of the `M_lmer` object

The mean of the random intercepts $\alpha$ was estimated as a global intercept
term.  If it's confusing why the random intercept mean $\mu_{\alpha}$ would
appear as a global intercept, just observe that the model definition could just
as well have been expressed as

$$ Y \mid X \sim N(\mu_{\alpha} + \alpha_{[i]}, \sigma I) $$
$$ \alpha \sim N(0, \sigma_{\alpha}) $$

which illistrates the general point that even the most simple multilevel models
may be expressed in many equivelent ways.  This global intercept term is our
first example of a *fixed effect*. 

```{r}
fixef(M_lmer)
```

Fixed effects have have no apriori assumed distribution, as opposed to *random
effects*, for which we do make such an assumption.

The collection of estimated random intercepts $\hat \alpha$ caa be retrieved
using the `ranef` acessor function

```{r}
head(ranef(M_lmer)$X)
```

The estimated standard deviation of these intercepts $\hat \sigma_{\alpha}$ can
be accessed as

```{r}
attributes(VarCorr(M_lmer)$X)$stddev
```

and the residual standard deviation $\hat \sigma$ as

```{r}
sigma(M_lmer)
```

From these values, it looks like `lmer` got the model quite right.  The true
model is

$$ Y \mid X \sim N(0, 1) $$

so the correct parameters would be $\hat \alpha_i = 0$ for all $i$, $\hat
\sigma_{\alpha} = 0$, and $\hat \sigma = 1$.  We were very close [^1]!

To make sure we didn't just get lucky, we can repeat this entire precodeure
many times, and then plot the resulting histograms of estimated random effects

```{r echo=FALSE}
set.seed(154)
```

```{r message=FALSE, warning=FALSE}
estiamte_lmer_random_classes <- function(N, N_classes) {
  df <- data.frame(
    x = factor(sample(1:N_classes, N, replace=TRUE)),
    y = rnorm(N)
  )
  M <- lmer(y ~ (1 | x), data = df)
  ranef(M)$x[, 1]
}

model_coefs_melted <- replicate_models(10000, 100,
                                      estiamte_lmer_random_classes, 25)
ggplot(data=model_coefs_melted, aes(x=coef)) +
  geom_histogram() +
  facet_wrap(~model_id) +
  labs(title="Parameter Estimates in Null Random Intercept Model")
```

The random intercept model does much better than the linear model.  It often
finds the exact truth ($\hat \sigma_{\alpha} = 0$), and otherwise assigns a
very small standard deviation to the distribution of intercepts.

To emphasize the point, Here's a side by size comparison of the intercepts in
one of the models returned from `lme` vs the same returned from `lmer`

```{r message=FALSE, warning=FALSE}
set.seed(154)
df <- data.frame(
  x = factor(sample(1:N_classes, N, replace=TRUE)),
  y = rnorm(N)
)
M_lm <- lm(y ~ x, data = df)
M_lmer <- lmer(y ~ (1 | x), data = df)
plot_data <- data.frame(coef = c(coef(M_lm), ranef(M_lmer)$x[, 1]),
                        type = rep(c("lm", "lmer"), each = N_classes))
ggplot(aes(x = coef, fill = type), data = plot_data) + 
  geom_histogram() + 
  facet_wrap(~ type) +
  labs(title="Linear and Random Intercept Null Model Coefficients (Same Scale)")
```

The random intercept controls the untamed varaince of the linear model.

[^1]: Of course, we cant say that we were "close" in an absolute sense, but the
ratio $\frac{\sigma_{\alpha}}{\sigma}$ of the group standard deviation to the
residual standard deviation *is* meaningful, and the model did well in this
respect.

Fixed Effect Terms
------------------

Above we saw that the intercept in a random intercept model is estimted as a
*fixed effect*.  We can add other fixed effects to a multilevel model.

For example, we can estimate a model with an intercept and linear term as fixed
effects, along with a random intercept.  In detail, we can write out this model
as

$$ Y \mid X \sim N(\beta_0 + \beta_1 x + \alpha_{[i]}, \sigma) $$
$$ \alpha \sim N(0, \sigma_{\alpha}) $$

Note that this is completely equivelent to

$$ Y \mid X \sim N( \alpha_{[i]}, \sigma) $$
$$ \alpha \sim N(\beta_0 + \beta_1 x, \sigma_{\alpha}) $$

which again illistrates a multilevel model can be expressed in multiple ways.

This fixed intercept and slope, random intercept model is also easy to estiamte
with `lmer`

```{r}
set.seed(154)
df <- data.frame(
  t = runif(N),
  X = factor(sample(1:N_classes, N, replace=TRUE))
)
df$y <- df$t + rnorm(N)
M_lmer <- lmer(y ~ t + (1 | X), data = df)
```

Now there is both a fixed effect intercept and a fixed effect slope

```{r}
fixef(M_lmer)
```

We can check out the random effects just like in the simpler model

```{r}
attributes(VarCorr(M_lmer)$X)$stddev
```

```{r}
sigma(M_lmer)
```

Again, it looks like this model captures the essence of the situation.  We can
again simulate many times to be sure

```{r echo=FALSE}
estimate_lmer_with_fixed <- function(N, N_classes) {
  df <- data.frame(
    t = runif(N),
    x = factor(sample(1:N_classes, N, replace=TRUE))
  )
  df$y <- df$t + rnorm(N)
  M <- lmer(y ~ t + (1 | x), data = df)
  M
}

set.seed(154)
N_replicates <- 9
N_classes <- 100
lmers_fixed_effects <- replicate(
  N_replicates,
  fixef(estimate_lmer_with_fixed(10000, N_classes))
)

# Evaluate the fixed effects part of the model on a grid of points on [0, 1]
N_grid_points <- 4
grid_matrix <- cbind(rep(1, N_grid_points), seq(0, 1, length = N_grid_points))
fixef_preds <- grid_matrix %*% lmers_fixed_effects
fixef_preds_melted <- data.frame(
  model_id = factor(rep(1:N_replicates, each = N_grid_points)),
  x = rep(seq(0, 1, length = N_grid_points), N_replicates),
  fix_effects_preds = as.vector(fixef_preds)
)
p_fixef <- ggplot(data = fixef_preds_melted, aes(x = x, y = fix_effects_preds)) +
                  geom_line(aes(group = model_id), alpha = .25) +
                  geom_abline(intercept = 0, slope = 1, color="red") +
                  labs(title="Fix Effects Estiamtes")


# Plot the random effect distribution from the same model
set.seed(154)
lmers_rand_effects <- replicate(
  N_replicates,
  ranef(estimate_lmer_with_fixed(10000, N_classes))$x[,1]
)
rand_effects_melted <- data.frame(
  model_id = rep(1:N_replicates, each = N_classes),
  coef = as.vector(lmers_rand_effects)
)
p_ranef <- ggplot(data=rand_effects_melted, aes(x=coef)) +
           geom_histogram() +
           facet_wrap(~model_id) +
           labs(title="Random Effects Estimates")
```

```{r message=FALSE, warning=FALSE}
# I've suppresed the busywork involved in making these plots.
library("gridExtra")
grid.arrange(p_fixef, p_ranef, ncol=2)
```

It should be clear at this point that would could generalize to *random
slopes*, experimentation here is left to the reader.

Nested Classes
--------------

We still have not justified the name "Multilevel Models" (sometimes the term
"Heiarchical Models" is also used, or "Mixed-Effects Models", the naming is
confusing).

To understand why "Multilevel Models" is a good name, lets construct some data
with two types of predictors:

  - `class` is a catagorical predictor with `N_class` levels.
  - `subclass` is a another catagorical predictor that bears a parent-child
    relationship to `class`.  That is, each level of `class` has `N_subclass`
    associated levels of `subclass`.
  
Many times, in situations like this, we would expect the influence of
`subclass` on some response varaible to be best expressed an adjustment to the
influence of `class`.  Said another way, the levels of `class` may act as
cluster labels for the levels of `subclass` within the context of a regression.

We can express this idea in the context of a miltilevel model by hypothesizing
the following data model

$$ Y \mid X \sim N(\mu_{\text{class}} + \alpha_{[i]} + \beta_{[ij]}, \sigma) $$
$$ \alpha \sim N(0, \sigma_{\alpha}) $$
$$ \beta_{[i*]} \sim N(0, \sigma_{\alpha_{[i]}}) $$

The notation is starting to get difficult[^2].  Here $\alpha$ represents the
`class` random intercepts, and $\beta$ represents the `subclass` random
intercepts.  Informally, we are saying that the `class` intercepts should
cluster around the fixed intercept, and the `subclass` intercepts should
cluster around the corresponding parent `class` intercept.

Let's generate some data using this scheme.  First, `class` parameters[^3]

```{r}
N_class <- 9

class_parameters <- structure(
  rnorm(N_class, sd = 3),
  names = as.character(1:N_class)
)

head(class_parameters)
```

and then subclass parameters

```{r}
N_subclass <- 100
subclass_parameters <- structure(
  rnorm(N_class*N_subclass),
  # The transpose is to put things in an inuitive order
  names = as.vector(t(outer(1:N_class, 1:N_subclass, paste, sep=".")))
)
head(subclass_parameters)
```

We call the sum of a `class` and `subclass` parameter an `effective` parameter
(this is not a common term, but is useful for this discussion), it is this
value that forms the total effect size for the subclass.  We can visualize the
distribution of effective parameters in our model, grouped by thier associated
class

```{r meessage=FALSE, warning=FALSE}
# Get the class name from a subclass name. For example:
#   get_class_name("10.4") == "10"
get_class_name <- function(subclass_name) {
  strsplit(subclass_name, ".", fixed = TRUE)[[1]][1]
}

effective_parameters <- lapply(
  names(subclass_parameters),
  function(x) subclass_parameters[x] + class_parameters[get_class_name(x)]
)
names(effective_parameters) <- names(subclass_parameters)

parameters_df <- data.frame(
  effective_parameter = unlist(effective_parameters),
  class = rep(names(class_parameters), each = N_subclass),
  class_parameter = rep(class_parameters, each = N_subclass)
)

ggplot(data = parameters_df, aes(x = effective_parameter)) +
  geom_density(aes(fill = class), alpha = .25) +
  geom_rug(aes(x = class_parameter, colour = class)) +
  labs(title = "Distribution of True Effective Parameters")
```

It may be easier to visualize if we break out the individual subclass parameter
distributions into seperate density plots

```{r meessage=FALSE, warning=FALSE}
ggplot(data = parameters_df, aes(x = effective_parameter)) +
  geom_density(aes(fill = class), alpha = .25) +
  geom_rug(aes(x = class_parameter, colour = class)) +
  facet_wrap(~ class) +
  labs(title = "Distribution of True Effective Parameters") 
```

The `lme4` library can fit the parameters in this model, justifying the choice
of name.  First, let's generate some response data

Each group of `subclass` parameters takes a bell shaped distribution around the
`class` mean (which are marked on the rug plot below the x-axis).

```{r}
N_obs <- 20000

class <- as.character(sample(1:N_class, N_obs, replace = TRUE))
subclass <- as.character(sample(1:N_subclass, N_obs, replace = TRUE))
class_subclass <- paste(class, subclass, sep = ".")
y <- class_parameters[class] + subclass_parameters[class_subclass] + rnorm(N_obs)

class_subclass_df <- data.frame(
  class  = class,
  subclass = subclass,
  y = y
)
head(class_subclass_df)
```

The multilevel model we layed out above can be fit using the `:` operator in
`lme4` to express the parent-child relationship of `class` and `subclass`

```{r}
class_subclass_lmer <- lmer(y ~ (1 | class) + (1 | class:subclass),
                            data = class_subclass_df)
```

Breaking this down

  - The implied `1` fits a fixed effect overall intercept.
  - The `(1 | class)` term fits a random intercept for `class`.
  - The `(1 | class:subclass)` fits a random intercept for `subclass`, but with
  each group of `subclass` estiamtes centered around the estiamted intercept for the parent `class`.

Let's look into how well this model captures reality.  First, we plot the
estimated and actual class means, along with thie standard error of each
estimated class mean

```{r message=FALSE, warning=FALSE}
# TODO: Make this a one df plot, so that the labels play nice.
library("arm") # Contains extra functions for manipulating lmer objects.
class_means_df <- data.frame(
  class_id = names(class_parameters),
  true_class_means = class_parameters,
  estimated_class_means = fixef(class_subclass_lmer) + 
                          ranef(class_subclass_lmer)$class[, 1],
  # The intercept and class means are assumed to vary independently here.
  upper_se = fixef(class_subclass_lmer) + 
             ranef(class_subclass_lmer)$class[, 1] +
             sqrt(se.coef(class_subclass_lmer)$fixef^2 + 
                          se.coef(class_subclass_lmer)$class[, 1]^2),
  lower_se = fixef(class_subclass_lmer) +
             ranef(class_subclass_lmer)$class[, 1] -
             sqrt(se.coef(class_subclass_lmer)$fixef^2 + 
                          se.coef(class_subclass_lmer)$class[, 1]^2)
)

ggplot(data = class_means_df) +
  geom_point(aes(x = class_id, y = estimated_class_means, colour = "estimated")) +
  geom_pointrange(aes(x = class_id, y = estimated_class_means,
                      ymax = upper_se, ymin = lower_se, colour = "estimated"),
                  alpha = .5) +
  geom_point(aes(x = class_id, y = true_class_means, colour = "actual")) +
  scale_colour_manual(name = "type", values = c("black", "blue")) +
  labs(title = "Estiamted vs. Actual Class Means")
```

We can also recrate the above density plots of the `subclass` parameters, this
time using the *estimated*, instead of true, values

```{r message=FALSE, warning=FALSE}
estimated_parameters_df <- data.frame(
  estimated_effective_parameter =
    rep(fixef(class_subclass_lmer), N_class*N_subclass) +
    rep(ranef(class_subclass_lmer)$class[, 1], each = N_subclass) +
    ranef(class_subclass_lmer)[["class:subclass"]][, 1],
  class = factor(rep(1:9, each = N_subclass))
)

ggplot(data = estimated_parameters_df, aes(x = estimated_effective_parameter)) +
  geom_density(aes(fill = class), alpha=.25) +
  labs(title = "Distribution of Estimated Effective Parameters")
```

Here that is side by side with the actual values

```{r message=FALSE, warning=FALSE}
actuals <- ggplot(data = parameters_df, aes(x = effective_parameter)) +
  geom_density(aes(fill = class), alpha=.25)
estimates <- ggplot(data = estimated_parameters_df,
                    aes(x = estimated_effective_parameter)) +
  geom_density(aes(fill = class), alpha = .25)
grid.arrange(actuals, estimates, nrow=2)
```

The multilevel model has done a great job capturing all the features of our
data!

It's worth comparing this to what we would get using only a classical linear
model

```{r}
subclass_lm <- lm(y ~ factor(subclass)*factor(class), data = class_subclass_df)
estimated_parameters_lm <- coef(subclass_lm)
# Assign reasonable names to the coeffieients in the correct order.
names(estimated_parameters_lm) <- as.vector(
  t(outer(1:N_class, 1:N_subclass, paste, sep="."))
)
subclass_lm_df <- data.frame(
  estimated_parameter = estimated_parameters_lm,
  class = rep(names(class_parameters), each = N_subclass)
)
ggplot(data = subclass_lm_df, aes(x = estimated_parameter)) + 
  geom_density(aes(fill = class), alpha = .25) +
  labs(title = "Distribution of Linear Model Parameters")
```

Recall that the *within* class variance of the true parameters is always $1$ in
our data.  The classical linear model allocates too much of the parameter
variance *within* each class and too little *between* the classes.  Observe
that the class means themselves are hardly distinguished at all!

[^2]: It always does, eventually.  To make it worse, here's yet another way to
express the model:
$$ Y \mid X \sim N(\beta_{[ij]}, \sigma) $$
$$ \alpha \sim N(\mu_{\alpha}, \sigma_{\alpha}) $$
$$ \beta_{[i*]} \sim N(\alpha_{[i]}, \sigma_{\alpha_{[i]}}) $$

[^3]: We limit to $9$ class parameters to assist with visualizations.
